\input{../preamble_beamer.tex}

%----------------------------------------------------------------------------------------------------
%--------- Document Title ---------------------------------------------------------------------
\title{Lineare Algebra\\[3mm] 
	\large Diagonalisierbarkeit
}
\author{Dr. Moritz Gruber} 
\institute{DHBW Karlsruhe}
\date{2022}
%%%%%%%%%%%%%%
\begin{document}

\AtBeginSection[]{
	\begin{frame}				
		\usebeamercolor[fg]{frametitle}
		{\Large \insertsection} 
        \end{frame}
}

%
\begin{frame}[plain] 
 \titlepage
\end{frame}
%
%
\begin{frame}\frametitle{Inhalt}
   \tableofcontents
\end{frame}
%
%%%
\section{Diagonalmatrizen}
%%%
\subsection{Eigenschaften von Diagonalmatrizen}
%
\begin{frame}\frametitle{Eigenschaften von Diagonalmatrizen}

\highlightDef{Erinnerung:} Eine Diagonalmatrix $A=diag(\alpha_1,...,\alpha_n) \in \R^{n\times n}$ ist eine quadratische Matrix der Gestalt
$$
diag(\alpha_1,...,\alpha_n)=\begin{pmatrix} \alpha_1 & 0& 0&... & 0 \\0 & \alpha_2 & 0 & ... & 0 \\ ...&...&...&...&... \\ ...&...&0&\alpha_{n-1}&0 \\ 0 & ... &0 & 0 & \alpha_n \end{pmatrix}
$$\pause
Wir haben gehen, dass Diagonalmatrizen gute Eigenschaften haben:\pause
\begin{itemize}
\item $\det(diag(\alpha_1,...,\alpha_n))= \prod_{i=0}^n \alpha_i$ \pause
\item Falls $\det(diag(\alpha_1,...,\alpha_n)) \ne 0$, dann gilt $diag(\alpha_1,...,\alpha_n)^{-1}=diag(\alpha_1^{-1},...,\alpha_n^{-1})$.
\end{itemize}
	
\end{frame}
%
%
\subsection{Diagonalisierbare Matrizen}
%
\begin{frame}\frametitle{Definition: Diagonalisierbarkeit}
Es sei $n \in \N$ und $A \in \R^{n\times n}$ eine Matrix. Dann heißt $A$ \highlightDef{diagonalisierbar}, wenn es eine invertierbare Matrix $D \in \R^{n\times n}$ gibt, sodass $D^{-1}\cdot A \cdot D$ eine Diagonalmatrix ist.\\\pause \vfill
Andersherum betrachtet lässt sich eine diagonalisierbare Matrix $A$ schreiben als
$$
A=D\cdot diag(\alpha_1,...,\alpha_n)\cdot D^{-1}
$$

\pause\vfill
Wir werden sehen, dass diagonalisierbare Matrizen ähnlich gute Eigenschaften wie Diagonalmatrizen haben.\\\vspace{5mm}
Man muss nur ihre \highlightDef{Diagonalform} und die \highlightDef{Transformationsmatrix} $D$ kennen...
\end{frame}
%

\begin{frame}\frametitle{Eigenschaften diagonalisierbarer Matrizen}
Wenn $A \in \R^{n\times n}$ diagonalisierbar ist, dann gibt es Elemente $\alpha_1,...,\alpha_n \in \R$ und eine invertierbare Matrix $D \in \R^{n\times n}$, sodass $D^{-1}AD=diag(\alpha_1,...,\alpha_n)$. Außerdem gilt dann:\\\pause\vfill
\begin{itemize}
\item $\det(A)=\prod_{i=1}^n \alpha_i$\\\pause\vspace{2mm}
Denn:\quad $\det(A)=\det(D\cdot diag(\alpha_1,...,\alpha_n)\cdot D^{-1})$\\\pause
\hspace{25mm}$=\det(D)\det(diag(\alpha_1,...,\alpha_n))\frac{1}{\det(D)}$\\\pause
\hspace{25mm}$=\prod_{i=1}^n \alpha_i$\\\pause
\vfill
%
\item $A^{-1}=D\cdot diag(\alpha_1^{-1},...,\alpha_n^{-1})\cdot  D^{-1}$\\\pause\vspace{2mm}
Denn:\quad $A^{-1}=(D\cdot diag(\alpha_1,...,\alpha_n)\cdot D^{-1})^{-1}$\\\pause
\hspace{21mm}$=(diag(\alpha_1,...,\alpha_n)\cdot D^{-1})^{-1}D^{-1}$\\\pause
\hspace{21mm}$=(D^{-1})^{-1}\cdot diag(\alpha_1,...,\alpha_n)^{-1}\cdot D^{-1}$\\\pause
\hspace{21mm}$=D\cdot diag(\alpha_1^{-1},...,\alpha_n^{-1})\cdot D^{-1}$
\end{itemize} 
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel}
Für $A=\begin{pmatrix}4&0&0\\1&2&0\\2&0&6 \end{pmatrix} \in \R^{3 \times 3}$  erhält man mit den Matrizen
$D=\begin{pmatrix}0&-2&0\\1&-1&0\\0&2&1 \end{pmatrix}$	
und
$D^{-1}=\begin{pmatrix}-\frac{1}{2}&1&0\\-\frac{1}{2}&0&0\\1&0&1 \end{pmatrix}$,\\\vfill
dass $A$ diagonalisierbar ist: \pause
$$
D^{-1}AD=\begin{pmatrix} 2 & 0&0 \\ 0&4&0 \\ 0&0&6 \end{pmatrix}=diag(2,4,6)
$$
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel (Fortsetzung)}
Außerdem gilt:\vfill
\begin{itemize}
\item $\det(A)=4\cdot2\cdot6=48=2\cdot4\cdot6=\det(diag(2,4,6))$ \vfill\pause
\item $A^{-1}=\begin{pmatrix}\frac{1}{4}&0&0\\-\frac{1}{8}&\frac{1}{2}&0\\-\frac{1}{12}&0&\frac{1}{6} \end{pmatrix}=D\cdot diag(\frac{1}{2},\frac{1}{4},\frac{1}{6})\cdot D^{-1}$
\end{itemize} \vfill\pause
\highlightDef{Fragen:}\quad Wann ist eine Matrix $A$ diagonalisierbar? \\
\hspace{15.75mm}Wie findet man im Allgemeinen die Matrix $D$? 
\end{frame}
%
%
\section{Eigenwerte und Eigenvektoren}
%
\subsection{Definition}
%
\begin{frame}\frametitle{Matrix-Vektor-Multiplikation mit Diagonalmatrizen}
Es sei $A=diag(\alpha_1,...,\alpha_n)\in \R^{n\times n}$ eine Diagonalmatrix und $e_1,...,e_n \in \R^n$ die Standardbasisvektoren.\pause Dann gilt:
$$
A\cdot e_j= \alpha_j \cdot e_j
$$\pause
Die Multiplikation des $j$-ten Standardbasisvektors mit der Diagonalmatrix $A$ skaliert also gerade diesen Vektor um den $j$-ten Diagonaleintrag der der Matrix.\\\vfill\pause
\highlightDef{Beispiel}\\
$A=diag(a,b,c) \in \R^{3 \times 3}$, dann:
\small$$
A\cdot e_1 = \begin{pmatrix} a & 0&0 \\ 0&b&0 \\ 0&0&c \end{pmatrix}\cdot \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}=\begin{pmatrix} a \\ 0 \\ 0 \end{pmatrix}, \ 
A\cdot e_2 = \begin{pmatrix} a & 0&0 \\ 0&b&0 \\ 0&0&c \end{pmatrix}\cdot \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}=\begin{pmatrix} 0 \\ b \\ 0 \end{pmatrix}$$
und
$A\cdot e_3 = \begin{pmatrix} a & 0&0 \\ 0&b&0 \\ 0&0&c \end{pmatrix}\cdot \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}=\begin{pmatrix} 0 \\ 0 \\ c \end{pmatrix}
$
\end{frame}
%%
\begin{frame}\frametitle{Definition}
Es sei $A\in\R^{n\times n}$ eine Matrix. \pause \vfill
\begin{itemize}
\item[a)] Ein Vektor $v \in \R^n\setminus \{0\}$ heißt \highlightDef{Eigenvektor} von $A$, wenn gilt: $A\cdot v = \lambda \cdot v$ für ein $\lambda \in \R$.\pause \vfill
\item[b)] Ein Wert $\lambda \in \R$ heißt \highlightDef{Eigenwert} von $A$, wenn es einen Eigenvektor $v$ von $A$ gibt mit $A\cdot v = \lambda\cdot v$.\pause \vfill
\item[c)] Die Menge aller Eigenwerte von $A$ nennt man das \highlightDef{Spektrum} von $A$, notiert als $Spec(A)$.
\end{itemize}
	
\end{frame}
%
%
\begin{frame}\frametitle{Satz}
Es sei $A\in \R^{n\times n}$ und $\lambda \in Spec(A)$. Die Menge aller Eigenvektoren von $A$ zum Eigenwert $\lambda$
$$
Eig(A,\lambda):=\{v \in \R^n \mid Av=\lambda v\}\subseteq \R^n
$$
ist ein Untervektorraum von $\R^n$. Man nennt ihn den \highlightDef{Eigenraum} zum Eigenwert $\lambda$.\\\vfill \pause
\highlightDef{Beweis}\\
Wir benutzen hierfür das Untervektorraumkriterium: \pause
\begin{itemize}
\item $Eig(A,\lambda) \ne \emptyset$, da $\lambda \in Spec(A)$. \pause
\item $\forall u,v \in Eig(A,\lambda): A(u+v)=Au +Av = \lambda u + \lambda v = \lambda (u+v)$ und somit $u+v \in Eig(A,\lambda)$.\pause
\item $\forall \alpha \in \R \ \forall v \in Eig(A,\lambda): A(\alpha v)=\alpha Av=\alpha \lambda v=\lambda(\alpha v)$ und somit $\alpha v \in Eig(A,\lambda)$.\pause
\end{itemize}	
Damit sind alle Voraussetzungen des Untervektorraumkriteriums erfüllt. \hfill $\square$
\end{frame}
%
%
\begin{frame}\frametitle{Basis aus Eigenvektoren}
Es sei $A \in \R^{n \times n}$ und $B=\{b_1,...,b_n\}$ eine Basis des $\R^n$ aus Eigenvektoren von $A$ zu Eigenwerten $\lambda_1,...,\lambda_n$. \\(Die Eigenwerte sind \textit{nicht} zwingend paarweise verschieden!)\\\pause
Weiter sei $D$ die Abbildungsmatrix der linearen Abbildung $\Phi: \R^n \to \R^n$ mit $\Phi(e_j)=b_j$.\pause\\ Dann gilt:
$$
D^{-1}AD\cdot e_j =D^{-1}A\cdot b_j=D^{-1}\cdot\lambda_j b_j=\lambda_j e_j \quad \forall j \in \{1,...,n\}.
$$\pause
Daraus folgt, dass $D^{-1}AD$ die Abbildungsmatrix der linearen Abbildung $\Psi:\R^n \to \R^n$ mit $\Psi(e_j)=\lambda_j e_j$ ist.\pause \\Da die Abbildungsmatrix einer linearen Abbildung eindeutig bestimmt ist, folgt:
$$
D^{-1}AD=diag(\lambda_1,...,\lambda_n)
$$
\end{frame}
%
%
\begin{frame}\frametitle{Eigenwertgleichung}
Es sei $A\in \R^{n\times n}$ und $\lambda \in Spec(A)$ sowie $v \in \R^n\setminus \{0\}$. Dann gilt:\\\vfill 
$
v \in Eig(A,\lambda)\pause \Longleftrightarrow Av=\lambda v \pause \Longleftrightarrow Av-\lambda v=0 \pause\Longleftrightarrow (A-\lambda I_n)v=0
$\pause
\vfill
D.h. Eigenvektoren von $A$ sind nicht-triviale Lösungen des homogenen LGS \highlightDef{$(A-\lambda I_n)v=0$}.\\\vfill\pause
Und Eigenwerte sind dann die Werte $\lambda$, für die das LGS $(A-\lambda I_n)v=0$ nicht-triviale Lösungen besitzt, d.h. nicht eindeutig lösbar ist. \pause Dies ist genau dann der Fall, wenn die Matrix $A-\lambda I_n$ \underline{nicht} invertierbar ist.
\end{frame}
%
%%
\subsection{Das charakteristische Polynom}
%%
\begin{frame}\frametitle{Definition}
Es sei $A \in \R^{n\times n}$. Das Polynom
$$
CP_A(X):=\det(A-X\cdot I_n)
$$
heißt das \highlightDef{charakteristische Polynom} von $A$.
\vfill \pause
Die Matrix $A-\lambda I_n$ ist genau dann nicht invertierbar, wenn $\lambda$ eine Nullstelle des charakteristischen Polynoms von $A$ ist. Daraus folgt:
$$
\lambda \in Spec(A) \Longleftrightarrow CP_A(\lambda)=0
$$
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel 1}
Sei $A=\begin{pmatrix} -1 & 0 & -2 \\ 0 & 3 & 0 \\ 2 & 0 &4 \end{pmatrix}$. \\Wir bestimmen das charakteristische Polynom $CP_A(X)=\det(A-X\cdot I_3)$:\\ \vfill\pause
$
\det(A-X\cdot I_3)=\det(\begin{pmatrix} -1-X & 0 & -2 \\ 0 & 3-X & 0 \\ 2 & 0 &4-X \end{pmatrix})$\\\pause
\hspace{24.5mm}$=(3-X)\cdot \det(\begin{pmatrix} -1-X &  -2 \\ 2 &4-X \end{pmatrix})$\\\pause
\hspace{24.5mm}$=(3-X)\cdot ((-1-X)(4-X)-(-2)\cdot2)$\\\pause
\hspace{24.5mm}$=(3-X)\cdot(-4-3X+X^2+4)$\\\pause
\hspace{24.5mm}$=(3-X)\cdot(X^2-3X)$\\\pause
\hspace{24.5mm}$=-(3-X)^2X
$
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel 1}
Damit erhalten wir $CP_A(X)=-(3-X)^2X$ mit Nullstellen $\lambda_1=3$ und $\lambda_2=0$. \\\pause
Nun bestimmen wir die Eigenräume. Dazu lösen wir die LGSe $(A-3I_3)v=0$ und $(A-0I_3)v=0$:
\begin{itemize}
\item[$\lambda_1$:]
$
\left(\begin{array}{rrr|r}
-1-3 & 0 & -2& 0 \\ 0 & 3-3 & 0& 0 \\ 2 & 0 &4-3& 0
\end{array}\right)
\sim >\pause
\left(\begin{array}{rrr|r}
-4 & 0 & -2& 0 \\ 0 & 0 & 0& 0 \\ 2 & 0 &1& 0
\end{array}\right)
$\\
$\stackrel{-\frac{1}{4}Z1,\ Z3-2Z1}{\sim >}\pause
\left(\begin{array}{rrr|r}
1 & 0 & \frac{1}{2}& 0 \\ 0 & 0 & 0& 0 \\ 0 & 0 &0& 0
\end{array}\right)
\stackrel{-1\text{-Trick}}{\sim >}\pause
\left(\begin{array}{rrr|r}
1 & \color{blue}{0} & \color{red}{\frac{1}{2}}& 0 \\ 0 & \color{blue}{-1} & \color{red}{0}& 0 \\ 0 & \color{blue}{0} &\color{red}{-1}& 0
\end{array}\right)
$
\end{itemize}
Damit ergibt sich $b_1=\begin{pmatrix} 0 \\ -1 \\ 0\end{pmatrix}$ und $b_2=\begin{pmatrix} \frac{1}{2} \\ 0 \\ -1\end{pmatrix}$ als Basis des Eigenraums $Eig(A,3)$.
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel 1}
\begin{itemize}
\item[$\lambda_2$:]
$
\left(\begin{array}{rrr|r}
-1-0 & 0 & -2& 0 \\ 0 & 3-0 & 0& 0 \\ 2 & 0 &4-0& 0
\end{array}\right)
\sim >\pause
\left(\begin{array}{rrr|r}
-1 & 0 & -2& 0 \\ 0 & 3& 0& 0 \\ 2 & 0 &4& 0
\end{array}\right)
$\\
$\stackrel{normieren}{\sim >}\pause
\left(\begin{array}{rrr|r}
1 & 0 & 2& 0 \\ 0 & 1 & 0& 0 \\ 1 & 0 &2& 0
\end{array}\right)
\stackrel{Z3-Z1}{\sim >}\pause
\left(\begin{array}{rrr|r}
1 & 0 & 2& 0 \\ 0 & 1 & 0& 0 \\ 0 & 0 &0& 0
\end{array}\right)$\\
$
\stackrel{-1\text{-Trick}}{\sim >}\pause
\left(\begin{array}{rrr|r}
1 & 0 & \color{red}{2}& 0 \\ 0 & 1 & \color{red}{0}& 0 \\ 0 & 0 &\color{red}{-1}& 0
\end{array}\right)
$
\end{itemize}
Damit erhalten wir als Basisvektor des Eigenraums $Eig(A,0)$ den Vektor $b_3=\begin{pmatrix} 2 \\ 0 \\ -1\end{pmatrix}$.
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel 1}
Die Vektoren $b_1=\begin{pmatrix} 0 \\ -1 \\ 0\end{pmatrix}$, $b_2=\begin{pmatrix} \frac{1}{2} \\ 0 \\ -1\end{pmatrix}$ und $b_3=\begin{pmatrix} 2 \\ 0 \\ -1\end{pmatrix}$ bilden nun eine Basis des $\R^3$ aus Eigenvektoren von $A$. Die Abbildungsmatrix der linearen Abbildung $\Phi: \R^3 \to \R^3$ mit $\Phi(e_j)=b_j$ ist 
$$
D=\begin{pmatrix} 0 & \frac{1}{2} & 2\\ -1 & 0 & 0 \\ 0 & -1 & -1\end{pmatrix} \ \text{ mit Inversen } D^{-1}=\begin{pmatrix}0&-1&0 \\ -\frac{2}{3} & 0& -\frac{4}{3} \\ \frac{2}{3} &0 &\frac{1}{3} \end{pmatrix}
$$\vfill
Damit folgt $D^{-1}AD=diag(3,3,0)$.
\end{frame}
%
%
\begin{frame}\frametitle{Beispiel 2}

Sei $A=\begin{pmatrix} 1 & 0 & 0 \\ 0 & -2 & 1 \\ 0 & -8 &2 \end{pmatrix}$. \\Wir bestimmen das charakteristische Polynom $CP_A(X)=\det(A-X\cdot I_3)$:\\ \vfill\pause
$
\det(A-X\cdot I_3)=\det(\begin{pmatrix} 1-X & 0 & 0 \\ 0 & -2-X & 1 \\ 0 & -8 &2-X \end{pmatrix})$\\\pause
\hspace{24.5mm}$=(1-X) \cdot \det(\begin{pmatrix}  -2-X & 1 \\  -8 &2-X \end{pmatrix})$\\\pause
\hspace{24.5mm}$=(1-X)\cdot((-2-X)(2-X)-(1\cdot (-8)))$\\\pause
\hspace{24.5mm}$=(1-X) (X^2-4+8)$\\\pause
\hspace{24.5mm}$=(1-X) (X^2+4)\pause
$\\\vfill
Der Faktor $X^2+4$ hat keine rellen Nullstellen und wir werden sehen, dass der Eigenraum zum einzigen Eigenwert $\lambda=1$ nur 1-dimensional ist. Somit lässt sich für $A$ keine Basis des $\R^n$ aus Eigenvektoren finden.	
\end{frame}
%
\begin{frame}\frametitle{Beispiel 2}
Um den Eigenraum zu $\lambda=1$ zu bestimmen, lösen wir das LGS $(A-I_3)v=0$:\\\vfill \pause
$
\left(\begin{array}{rrr|r}
1-1 & 0 & 0& 0 \\ 0 & -2-1 & 1& 0 \\ 0 & -8 &2-1& 0
\end{array}\right)
\sim >\pause
\left(\begin{array}{rrr|r}
0 & 0 & 0& 0 \\ 0 & -3 & 1& 0 \\ 0 & -8 &1& 0
\end{array}\right)$\\\vspace{3mm}$
\stackrel{-\frac{1}{3}Z2,\ Z3+8Z2}{\sim >}\pause
\left(\begin{array}{rrr|r}
0 & 0 & 0& 0 \\ 0 & 1 & -\frac{1}{3}& 0 \\ 0 & 0 &-\frac{5}{3}& 0
\end{array}\right)
\stackrel{-1\text{-Trick}}{\sim >}\pause
\left(\begin{array}{rrr|r}
\color{red}{-1} & 0 & 0& 0 \\ \color{red}{0} & 1 & -\frac{1}{3}& 0 \\ \color{red}{0} & 0 &1& 0
\end{array}\right)
$
\vfill
Damit ergibt sich $Eig(A,1)=\langle \{\begin{pmatrix}-1 \\ 0\\0 \end{pmatrix}\} \rangle$ und $\dim(Eig(A,1))=1<3=\dim(\R^3)$.
\end{frame}
%
\begin{frame}\frametitle{Satz}
Es sei $A \in \R^{n\times n}$. Dann ist $A$ genau dann diagonalisierbar, wenn es eine Basis des $\R^n$ aus Eigenvektoren von $A$ gibt.\\

\end{frame}
%

\end{document}